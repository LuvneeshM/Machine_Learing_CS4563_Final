{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# imports\nimport json\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport os\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn.preprocessing\nfrom sklearn import linear_model\nimport sklearn.model_selection\nfrom sklearn import preprocessing\nfrom sklearn import svm\nfrom sklearn.metrics import precision_recall_fscore_support\nimport keras\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Activation\nimport keras.backend as K\nfrom keras import optimizers\nfrom keras.models import load_model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7601bb30cb9bd913fbf1c511b315ff4bc9af3280"
      },
      "cell_type": "code",
      "source": "K.clear_session()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a17ea1e8422740ebf5edbd6c8c0e9b4edc065e7"
      },
      "cell_type": "code",
      "source": "class LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.loss = []\n        self.val_acc = []\n        \n    def on_batch_end(self, batch, logs={}):\n        self.loss.append(logs.get('loss'))\n        \n    def on_epoch_end(self, epoch, logs):\n        self.val_acc.append(logs.get('val_acc'))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "df6dce3ed828d677d98070c284ef196d4163a03b"
      },
      "cell_type": "code",
      "source": "# def ldf(directory, rows, skip=0):\n#     df = pd.read_csv(\"../input/train_v2.csv\",\n#                             nrows = rows,\n#                             skiprows = skip\n#                     )\n#     df = df.drop([\"date\", \"socialEngagementType\", \"visitStartTime\", \"visitId\", \"fullVisitorId\",\"hits\"], axis=1)\n#     devices_df = pd.DataFrame(df.device.apply(json.loads).tolist())[[\"browser\", \"operatingSystem\", \"deviceCategory\", \"isMobile\"]]\n#     geo_df = pd.DataFrame(df.geoNetwork.apply(json.loads).tolist())[[\"continent\", \"subContinent\", \"country\", \"city\"]]\n#     traffic_source_df = pd.DataFrame(df.trafficSource.apply(json.loads).tolist())[[\"keyword\", \"medium\", \"source\"]]\n#     totals_df = pd.DataFrame(df.totals.apply(json.loads).tolist())[[\"transactionRevenue\", \"newVisits\", \"bounces\", \"pageviews\", \"hits\"]]\n#     df = pd.concat([df, devices_df, geo_df, traffic_source_df, totals_df], axis=1)\n#     df = df.drop([\"device\", \"geoNetwork\", \"trafficSource\", \"totals\"], axis=1)\n#     df[\"transactionRevenue\"] = df[\"transactionRevenue\"].fillna(0)\n#     df[\"bounces\"] = df[\"bounces\"].fillna(0)\n#     df[\"pageviews\"] = df[\"pageviews\"].fillna(0)\n#     df[\"hits\"] = df[\"hits\"].fillna(0)\n#     df[\"newVisits\"] = df[\"newVisits\"].fillna(0)\n    \n#     df[\"transactionRevenue\"] = df[\"transactionRevenue\"].astype(np.float)\n    \n#     cat_features = ['channelGrouping', 'browser', 'operatingSystem', 'deviceCategory', 'isMobile',\n#                         'continent', 'subContinent', 'country', 'city', 'keyword', 'medium', \n#                         'source', 'customDimensions'] #strings -> need to map to floats?\n    \n#     for c in cat_features:\n#         le = preprocessing.LabelEncoder()\n#         le.fit(list(df[c].values.astype(\"str\")))\n#         df[c] = le.transform(list(df[c].values.astype(\"str\")))\n    \n#     numerical_features = ['visitNumber', 'newVisits', 'bounces', 'pageviews', \"hits\"]\n#     for c in numerical_features:\n#         df[c] = df[c].astype(np.float)\n    \n# #     df = df.loc[df['transactionRevenue'] != 0.0]\n    \n#     return df\n\n\ndef ldf(directory, rows, skip=0):\n    df = pd.read_csv(\"../input/train_v2.csv\",\n                            nrows = rows,\n                            skiprows = skip\n                    )\n    df = df.drop([\"date\", \"socialEngagementType\", \"visitStartTime\", \"visitId\", \"fullVisitorId\",\"hits\"], axis=1)\n    try:\n        devices_df = pd.DataFrame(df.device.apply(json.loads).tolist())[[\"browser\", \"operatingSystem\", \"deviceCategory\", \"isMobile\"]]\n    except:\n        zero_data = np.zeros(shape=(rows,len([\"browser\", \"operatingSystem\", \"deviceCategory\", \"isMobile\"])))\n        devices_df = pd.DataFrame(zero_data, columns=[\"browser\", \"operatingSystem\", \"deviceCategory\", \"isMobile\"])\n    try:\n        geo_df = pd.DataFrame(df.geoNetwork.apply(json.loads).tolist())[[\"continent\", \"subContinent\", \"country\", \"city\"]]\n    except:\n        zero_data = np.zeros(shape=(rows,len([\"continent\", \"subContinent\", \"country\", \"city\"])))\n        geo_df = pd.DataFrame(zero_data, columns=[\"continent\", \"subContinent\", \"country\", \"city\"])\n    try:\n        traffic_source_df = pd.DataFrame(df.trafficSource.apply(json.loads).tolist())[[\"keyword\", \"medium\", \"source\"]]\n    except:\n        zero_data = np.zeros(shape=(rows,len([\"keyword\", \"medium\", \"source\"])))\n        traffic_source_df = pd.DataFrame(zero_data, columns=[\"keyword\", \"medium\", \"source\"])\n    try:\n        totals_df = pd.DataFrame(df.totals.apply(json.loads).tolist())[[\"transactionRevenue\", \"newVisits\", \"bounces\", \"pageviews\", \"hits\"]]\n    except:\n        zero_data = np.zeros(shape=(rows,5))\n        totals_df = pd.DataFrame(zero_data, columns=[\"transactionRevenue\", \"newVisits\", \"bounces\", \"pageviews\", \"hits\"])\n    \n    df = pd.concat([df, devices_df, geo_df, traffic_source_df, totals_df], axis=1)\n    df = df.drop([\"device\", \"geoNetwork\", \"trafficSource\", \"totals\"], axis=1)\n    df[\"transactionRevenue\"] = df[\"transactionRevenue\"].fillna(0)\n    df[\"bounces\"] = df[\"bounces\"].fillna(0)\n    df[\"pageviews\"] = df[\"pageviews\"].fillna(0)\n    df[\"hits\"] = df[\"hits\"].fillna(0)\n    df[\"newVisits\"] = df[\"newVisits\"].fillna(0)\n    \n    df[\"transactionRevenue\"] = df[\"transactionRevenue\"].astype(np.float)\n    \n    cat_features = ['channelGrouping', 'browser', 'operatingSystem', 'deviceCategory', 'isMobile',\n                        'continent', 'subContinent', 'country', 'city', 'keyword', 'medium', \n                        'source', 'customDimensions'] #strings -> need to map to floats?\n    \n    for c in cat_features:\n        le = preprocessing.LabelEncoder()\n        le.fit(list(df[c].values.astype(\"str\")))\n        df[c] = le.transform(list(df[c].values.astype(\"str\")))\n    \n    numerical_features = ['visitNumber', 'newVisits', 'bounces', 'pageviews', \"hits\"]\n    for c in numerical_features:\n        df[c] = df[c].astype(np.float)\n        \n    df_0 = df.loc[(df['transactionRevenue'] == 0.0)]\n    df_0s = df_0.sample(frac=0.75)\n    df = df.loc[(df['transactionRevenue'] != 0.0)]\n    df = df.append(df_0s, ignore_index=True)\n#     r = random.randint(0,9)\n#     print(\"rand\",r)\n#     df = df.loc[(df['transactionRevenue'] != 0.0) | (random.randint(0,9) > 0)]\n#     print(df.shape)\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "953e9218a42712030d1aa9749f06c6115593b5d4"
      },
      "cell_type": "markdown",
      "source": "<h2>First Attempt</h2>"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c05b4204393f3a05c202156255aa4d2c41b838a0"
      },
      "cell_type": "code",
      "source": "K.clear_session()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d789e53bfdd27e1bf41231fea15adafc19262e6"
      },
      "cell_type": "code",
      "source": "%%time\nK.clear_session()\nnumber_rows_read = 0\nrows = 30000\nscores = []\naccuracy = []\n\nwhile (number_rows_read < 90000):\n    dftrain = ldf('../input/train_v2.csv', rows, list(range(1,number_rows_read)))\n    dftest = ldf('../input/test_v2.csv', rows, list(range(1,number_rows_read)))\n    \n    y_tr = np.log1p(np.array(dftrain[\"transactionRevenue\"]))\n    y_tr = y_tr.reshape(y_tr.shape[0],1)\n    dftrain = dftrain.drop([\"transactionRevenue\"], axis=1)\n    x_tr = np.array(dftrain)\n    \n    y_test = np.log1p(np.array(dftest[\"transactionRevenue\"]))\n    y_test = y_test.reshape(y_test.shape[0],1)\n    dftest = dftest.drop([\"transactionRevenue\"], axis=1)\n    x_test = np.array(dftest)\n    \n    # scale the training/test data\n    Xtr_scale = (x_tr - np.mean(x_tr, axis = 0)) / np.std(x_tr, axis = 0)\n    Xts_scale = (x_test - np.mean(x_test, axis = 0)) / np.std(x_test, axis = 0)\n    \n    nin = x_tr.shape[1]\n    nh = 25\n    nout = int(np.max(y_tr)+1)\n    model = Sequential()\n    model.add(Dense(nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n    model.add(Dense(1, name='output')) \n\n    history_cb = LossHistory()\n    opt = optimizers.Adam(lr = 0.001)\n    model.compile(optimizer = opt, loss = 'mean_squared_error', metrics = ['accuracy'])\n\n\n    batch_size = 10\n    model.fit(Xtr_scale, y_tr, epochs = 10, batch_size = batch_size, validation_data = (Xts_scale, y_test), callbacks = [history_cb])\n    \n    number_rows_read += rows\n    \n    score, acc = model.evaluate(Xts_scale, y_test)\n    scores.append(score)\n    accuracy.append(acc)\n\nprint(\"scores\", scores)\nprint(\"accuracy\", accuracy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab3f104849dcb3ffd28b904dbd9a47ef95b82f9a"
      },
      "cell_type": "code",
      "source": "test_rows = 2500\nnumber_rows_read = 15000\ndftrain = ldf('../input/test_v2.csv',test_rows, list(range(1,number_rows_read)))\ndf = dftrain.loc[dftrain['transactionRevenue'] != 0.0]\n\ny_test = np.log1p(np.array(df[\"transactionRevenue\"]))\ny_test = y_test.reshape((y_test.shape[0],1))\ny_test = np.array([y_test[14]])\nx_test = np.array(df.drop([\"transactionRevenue\"], axis=1))\nx_test = np.array([x_test[14]])\n\nyhat = model.predict(x_test)\nprint(yhat)\nprint(y_test)\n\nprint(\"~\"*20)\ntest_rows = 2500\nnumber_rows_read = 15000\ndftrain = ldf('../input/test_v2.csv',test_rows, list(range(1,number_rows_read)))\ndf = dftrain.loc[dftrain['transactionRevenue'] != 0.0]\n\ny_test = np.log1p(np.array(df[\"transactionRevenue\"]))\ny_test = y_test.reshape((y_test.shape[0],1))\nx_test = np.array(df.drop([\"transactionRevenue\"], axis=1))\n\nyhat = model.predict(x_test)\nprint(np.mean((yhat - y_test) ** 2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e37f9e96c54ca49c5cedf3931d76dc1b804e15c6"
      },
      "cell_type": "code",
      "source": "print(model.summary())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dd8e51de6baf823859f9efcb75810d7572f03486"
      },
      "cell_type": "markdown",
      "source": "<h2>Using different activation functions</h2>"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8009fb344b4081adafdf5da1a150294b6072fb15"
      },
      "cell_type": "code",
      "source": "K.clear_session()\nscores = []\naccuracy = []\nactivations = ['sigmoid', 'tanh', 'relu']\nloss_hist = []\nval_acc_hist = []",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9d23b567feb93aabf1ab2b945f44374de1f947e",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "for a in activations:\n    rows = 90000\n    number_rows_read = 0\n\n    K.clear_session()\n\n    dftrain = ldf('../input/train_v2.csv', rows, list(range(1,number_rows_read)))\n    dftest = ldf('../input/test_v2.csv', rows, list(range(1,number_rows_read)))\n\n    y_tr = np.log1p(np.array(dftrain[\"transactionRevenue\"]))\n    y_tr = y_tr.reshape(y_tr.shape[0],1)\n    dftrain = dftrain.drop([\"transactionRevenue\"], axis=1)\n    x_tr = np.array(dftrain)\n\n    y_test = np.log1p(np.array(dftest[\"transactionRevenue\"]))\n    y_test = y_test.reshape(y_test.shape[0],1)\n    dftest = dftest.drop([\"transactionRevenue\"], axis=1)\n    x_test = np.array(dftest)\n\n    # scale the training/test data\n    Xtr_scale = (x_tr - np.mean(x_tr, axis = 0)) / np.std(x_tr, axis = 0)\n    Xts_scale = (x_test - np.mean(x_test, axis = 0)) / np.std(x_test, axis = 0)\n\n    nin = x_tr.shape[1]\n    nh = 25\n    model = Sequential()\n    model.add(Dense(nh, input_shape=(nin,), activation=a, name='hidden'))\n    model.add(Dense(1, name='output')) \n\n    history_cb = LossHistory()\n    opt = optimizers.Adam(lr = 0.001)\n    model.compile(optimizer = opt, loss = 'mean_squared_error', metrics = ['accuracy'])\n    \n    batch_size = 10\n    model.fit(Xtr_scale, y_tr, epochs = 10, batch_size = batch_size, validation_data = (Xts_scale, y_test), callbacks = [history_cb])\n\n    number_rows_read += rows\n\n    score, acc = model.evaluate(Xts_scale, y_test)\n    scores.append(score)\n    accuracy.append(acc)\n\n    loss_hist.append(history_cb.loss)\n    val_acc_hist.append(history_cb.val_acc)\n\nprint(\"scores\", scores)\nprint(\"accuracy\", accuracy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed7c2fd801c918ba448cccf85ed17b6d67d15466"
      },
      "cell_type": "code",
      "source": "epochs = []\nntr = Xtr_scale.shape[0]\nfor i in range(len(history_cb.loss)):\n    epochs.append(((i+1)*batch_size) / ntr)\nplt.semilogy(epochs, loss_hist[0], c=\"r\")\nplt.semilogy(epochs, loss_hist[1], c=\"b\")\nplt.semilogy(epochs, loss_hist[2], c=\"g\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")\nplt.grid()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a84b4033620f93ac5feee7a9a5d1ce911f8f032"
      },
      "cell_type": "code",
      "source": "plt.xlabel(\"activation\")\nplt.ylabel(\"score\")\nplt.scatter(activations, scores)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ed3a33866be956e66709cf0593601253611616b8"
      },
      "cell_type": "markdown",
      "source": "<h2>Using different number of hidden nodes</h2>"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4cfc22baa76db37faa7d24d0f22113a9f1eb9346"
      },
      "cell_type": "code",
      "source": "K.clear_session()\nscores = []\naccuracy = []\nhidden_nodes = [10, 25, 100]\nloss_hist_nodes = []\nval_acc_hist_nodes = []",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "301f919c6537dd0b016b1171e7236520265bc1a3",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "for n in hidden_nodes:\n    rows = 90000\n    number_rows_read = 0\n\n    K.clear_session()\n\n    dftrain = ldf('../input/train_v2.csv', rows, list(range(1,number_rows_read)))\n    dftest = ldf('../input/test_v2.csv', rows, list(range(1,number_rows_read)))\n\n    y_tr = np.log1p(np.array(dftrain[\"transactionRevenue\"]))\n    y_tr = y_tr.reshape(y_tr.shape[0],1)\n    dftrain = dftrain.drop([\"transactionRevenue\"], axis=1)\n    x_tr = np.array(dftrain)\n\n    y_test = np.log1p(np.array(dftest[\"transactionRevenue\"]))\n    y_test = y_test.reshape(y_test.shape[0],1)\n    dftest = dftest.drop([\"transactionRevenue\"], axis=1)\n    x_test = np.array(dftest)\n\n    # scale the training/test data\n    Xtr_scale = (x_tr - np.mean(x_tr, axis = 0)) / np.std(x_tr, axis = 0)\n    Xts_scale = (x_test - np.mean(x_test, axis = 0)) / np.std(x_test, axis = 0)\n\n    nin = x_tr.shape[1]\n    nh = 25\n    model = Sequential()\n    model.add(Dense(n, input_shape=(nin,), activation='relu', name='hidden'))\n    model.add(Dense(1, name='output')) \n\n    history_cb = LossHistory()\n    opt = optimizers.Adam(lr = 0.001)\n    model.compile(optimizer = opt, loss = 'mean_squared_error', metrics = ['accuracy'])\n    \n    batch_size = 10\n    model.fit(Xtr_scale, y_tr, epochs = 10, batch_size = batch_size, validation_data = (Xts_scale, y_test), callbacks = [history_cb])\n\n    number_rows_read += rows\n\n    score, acc = model.evaluate(Xts_scale, y_test)\n    scores.append(score)\n    accuracy.append(acc)\n\n    loss_hist.append(history_cb.loss)\n    val_acc_hist.append(history_cb.val_acc)\n\n\nprint(\"scores\", scores)\nprint(\"accuracy\", accuracy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5bef88d608b031d6dee57bd70bbe79ccf8a78013"
      },
      "cell_type": "code",
      "source": "epochs = []\nntr = Xtr_scale.shape[0]\nfor i in range(len(history_cb.loss)):\n    epochs.append(((i+1)*batch_size) / ntr)\nplt.semilogy(epochs, loss_hist[0], c=\"r\")\nplt.semilogy(epochs, loss_hist[1], c=\"b\")\nplt.semilogy(epochs, loss_hist[2], c=\"g\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")\nplt.grid()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c1a7b864881a3611c3b6548c85004b460e245ac"
      },
      "cell_type": "code",
      "source": "plt.xlabel(\"number nodes\")\nplt.ylabel(\"score\")\nplt.scatter(hidden_nodes, scores)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ab7199ffaf65a238edd3086caa2103bb37c41db"
      },
      "cell_type": "markdown",
      "source": "<h2>Using different learning rates</h2>"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "eba1932d23c104fae5501809b48537e74a0ae809"
      },
      "cell_type": "code",
      "source": "learning_rates = [0.01, 0.001, 0.0001]\nloss_hist_learn = []\nval_acc_hist_learn = []\nscores = []\naccuracy = []\n\n\nfor lr in learning_rates:\n    rows = 90000\n    number_rows_read = 0\n\n    K.clear_session()\n\n    dftrain = ldf('../input/train_v2.csv', rows, list(range(1,number_rows_read)))\n    dftest = ldf('../input/test_v2.csv', rows, list(range(1,number_rows_read)))\n\n    y_tr = np.log1p(np.array(dftrain[\"transactionRevenue\"]))\n    y_tr = y_tr.reshape(y_tr.shape[0],1)\n    dftrain = dftrain.drop([\"transactionRevenue\"], axis=1)\n    x_tr = np.array(dftrain)\n\n    y_test = np.log1p(np.array(dftest[\"transactionRevenue\"]))\n    y_test = y_test.reshape(y_test.shape[0],1)\n    dftest = dftest.drop([\"transactionRevenue\"], axis=1)\n    x_test = np.array(dftest)\n\n    # scale the training/test data\n    Xtr_scale = (x_tr - np.mean(x_tr, axis = 0)) / np.std(x_tr, axis = 0)\n    Xts_scale = (x_test - np.mean(x_test, axis = 0)) / np.std(x_test, axis = 0)\n\n    nin = x_tr.shape[1]\n    nh = 25\n    model = Sequential()\n    model.add(Dense(nh, input_shape=(nin,), activation='relu', name='hidden'))\n    model.add(Dense(1, name='output')) \n\n    history_cb = LossHistory()\n    opt = optimizers.Adam(lr = lr)\n    model.compile(optimizer = opt, loss = 'mean_squared_error', metrics = ['accuracy'])\n    \n    batch_size = 10\n    model.fit(Xtr_scale, y_tr, epochs = 10, batch_size = batch_size, validation_data = (Xts_scale, y_test), callbacks = [history_cb])\n\n    number_rows_read += rows\n\n    score, acc = model.evaluate(Xts_scale, y_test)\n    scores.append(score)\n    accuracy.append(acc)\n\n    loss_hist.append(history_cb.loss)\n    val_acc_hist.append(history_cb.val_acc)\n\n\nprint(\"scores\", scores)\nprint(\"accuracy\", accuracy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cbab71e39185335239766d14d082b0bcaf9e418f"
      },
      "cell_type": "code",
      "source": "plt.xlabel(\"learning rate\")\nplt.ylabel(\"score\")\nplt.semilogx(learning_rates, scores)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "488dda897c621e8c686a5652032aec2e43580fd4"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}